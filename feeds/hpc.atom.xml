<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Andrew Kail's Blog - HPC</title><link href="https://blog.kail.io/" rel="alternate"></link><link href="https://blog.kail.io/feeds/hpc.atom.xml" rel="self"></link><id>https://blog.kail.io/</id><updated>2023-12-08T00:00:00-05:00</updated><subtitle>Somewhat HPC related blog</subtitle><entry><title>Comparison of Provisioning/Cluster Managers in HPC</title><link href="https://blog.kail.io/comparison-of-provisioningcluster-managers-in-hpc.html" rel="alternate"></link><published>2023-12-08T00:00:00-05:00</published><updated>2023-12-08T00:00:00-05:00</updated><author><name>Andrew Kail</name></author><id>tag:blog.kail.io,2023-12-08:/comparison-of-provisioningcluster-managers-in-hpc.html</id><summary type="html">&lt;p&gt;We are primarily a &lt;del&gt;Bright Cluster Manager&lt;/del&gt; Base Command Manager (BCM) 
shop and recently some
uncertainty has arisen around the future of BCM after its purchase by Nvidia last year.
More specifically how 
pricing will be handled and whether BCM will only be available with Nvidia
superpods and DGX systems â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;We are primarily a &lt;del&gt;Bright Cluster Manager&lt;/del&gt; Base Command Manager (BCM) 
shop and recently some
uncertainty has arisen around the future of BCM after its purchase by Nvidia last year.
More specifically how 
pricing will be handled and whether BCM will only be available with Nvidia
superpods and DGX systems in the long term. &lt;/p&gt;
&lt;p&gt;As an HPC Managed Services Provider, we seek to leverage solutions that allow
us to quickly deploy and easily manage compute clusters. For our business operations, BCM 
has served us well in both the features it provides and the support from Nvidia.  Currently, all but
one HPC Cluster we manage runs BCM. &lt;/p&gt;
&lt;p&gt;Our lone outlier is a unique test case were we needed to manage both Linux
and Windows nodes in the same environment, so we instead went with another
provisioning software (Cobbler)[https://cobbler.github.io/] which had the ability
to support a Windows node.  We quickly however
realized there were many features missing we needed to fill via other means and the
Windows support proved more difficult that we like.&lt;/p&gt;
&lt;p&gt;In this article, I'll take a look at what the options are in the HPC space and
review the features needed in an a standalone deployment.  My goal in writing this is to 
understand the gaps in the solutions so our team, clients, and you, know
which ones need to be filled.&lt;/p&gt;
&lt;h2&gt;Provisioner vs Cluster Manager&lt;/h2&gt;
&lt;p&gt;First, I want to be clear about what is being compared here.  These tools will fall
into two categories, with one being a subset of the other.&lt;/p&gt;
&lt;p&gt;All of the software compared here will provide some form of provisioning. Provisioners
typically use some form of network booting combined with PXE and a TFTP server to handle installing and configuring an operating system to either baremetal hardware or a VM.  This has become the standard method for provisioning in both HPC and enterprise environments.&lt;/p&gt;
&lt;p&gt;The subcategory is cluster management software. A cluster managers are more specific to the HPC and parallel computing space. A cluster manager different from generic provisioning software in that it will typically run an agent on each node to manage and configure services for HPC.&lt;/p&gt;
&lt;h2&gt;The software&lt;/h2&gt;
&lt;p&gt;I'll be comparing the following software stacks.  This is not an exhaustive list of available software and I have decided to focus on what is either well supported, well known, or niche to this industry.&lt;/p&gt;
&lt;h3&gt;Provisioning Software&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://cobbler.github.io/"&gt;Cobbler&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cobbler is a provisioning software stack written in Python primarly geared towards
Linux network installations.  Its a very light install and leverages template and kickstart scripts for initial system configuration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://theforeman.org/"&gt;The Foreman&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Foreman is a complete lifecycle management tool for physical and virtual servers.
More than just a provisioner, it can also handle security update management,
configuraiton management, and monitoring. Even with its rich feature set I am leaving it
out of the Cluster Managers list as it is not geared towards HPC Clusters.&lt;/p&gt;
&lt;h3&gt;Cluster Managers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.nvidia.com/en-us/ai-data-science/products/base-command-manager-essentials/"&gt;Base Command Manager&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our story's main character.  Base Command Manager is a proprietary turn-key
application for installaing, configuring and managing and HPC cluster with
all the batteries included.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://warewulf.org/"&gt;Warewulf&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Warewulf is fast becoming one of the big players in the HPC Cluster management space.  It's the primary manager for the &lt;a href="https://openhpc.community/"&gt;OpenHPC Project&lt;/a&gt;, is open source with the option for enterprise support.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://grendel.readthedocs.io"&gt;Grendel&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Grendel is a cluster manager out of the University at Buffalo Center for Computational Research where it is used to manage their hpc resources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.penguinsolutions.com/computing/products/software/scyld-clusterware/"&gt;Scyld Clusterware&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scyld is a proprietary cluster management product from Penguin Solutions.  Development is based on the
beo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://xcat-docs.readthedocs.io/en/stable/"&gt;xCAT&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I well known management stack, xCAT is unfortunately no longer in development by IBM, but there is an attempt by the Open Source
Community to revive it.  However, I wanted to leave it in this list
as it is very well known and feature rich, albeit difficult to install and manage.&lt;/p&gt;
&lt;p&gt;Recently I did discover xCAT development has been continued by Lenovo under the name Confluent, albeit not
out in the open.  It can be found at &lt;a href="https://hpc.lenovo.com"&gt;hpc.lenovo.com&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://openhpc.community/"&gt;OpenHPC&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OpenHPC is a community effort to provide common HPC components which can be mixed and matched to build an cluster.  OpenHPC itself builds on top of Warewulf's
provisioning and provides other tools and libraries such as Infiniband, Lustre, and other scientific applications.&lt;/p&gt;
&lt;h2&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DHCP&lt;/li&gt;
&lt;li&gt;TFTP&lt;/li&gt;
&lt;li&gt;NTP&lt;/li&gt;
&lt;li&gt;IPMI configuration (During provisioning)&lt;/li&gt;
&lt;li&gt;Power control&lt;/li&gt;
&lt;li&gt;DNS&lt;/li&gt;
&lt;li&gt;Stateless provisioning&lt;/li&gt;
&lt;li&gt;Statefull provisioning&lt;/li&gt;
&lt;li&gt;Infiniband Driver&lt;/li&gt;
&lt;li&gt;Nvidia Drivers&lt;/li&gt;
&lt;li&gt;License&lt;/li&gt;
&lt;li&gt;Windows Provisioning&lt;/li&gt;
&lt;li&gt;User Management&lt;/li&gt;
&lt;li&gt;Firewall NAT Configuration&lt;/li&gt;
&lt;li&gt;Service Management (i.e. Systemd services)&lt;/li&gt;
&lt;li&gt;Image Management&lt;/li&gt;
&lt;li&gt;NFS Server&lt;/li&gt;
&lt;li&gt;Kernel Management&lt;/li&gt;
&lt;li&gt;Support Contract&lt;/li&gt;
&lt;li&gt;Slurm Management&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For many of these features, we are looking at what is provided by the provisioning software itself, not from the OS or
other related tools like Puppet, Chef, etc.  For example, The Foreman can tightly integrate with Puppet, but requires
more setup on the admin side to handle some of the features above.&lt;/p&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Cobbler&lt;/th&gt;
&lt;th&gt;The Foreman&lt;/th&gt;
&lt;th&gt;BCM&lt;/th&gt;
&lt;th&gt;Warewulf&lt;/th&gt;
&lt;th&gt;Grendel&lt;/th&gt;
&lt;th&gt;Scyld&lt;/th&gt;
&lt;th&gt;xCAT&lt;/th&gt;
&lt;th&gt;OpenHPC&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DHCP&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TFTP&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NTP&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IMPI Configuration&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Power Management&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DNS&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stateless provisioning&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Statefull provisioning&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Infiniband Drivers&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nvidia Drivers&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;License&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;td&gt;Proprietary&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;td&gt;Proprietary&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;td&gt;FOSS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Windows Provisioning&lt;/td&gt;
&lt;td&gt;Maybe&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;User Management&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Firewall NAT Configuration&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service Management&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image Management&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NFS Server&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kernel Management&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Support Contract&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Slurm Management&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;Its likely I missed some things and I will continue to update the comparison as time goes on or as people possibly yell at me.&lt;/p&gt;
&lt;p&gt;In the end, we have decided to plan migrations to OpenHPC as it covers the majority of what we need, has a robust community,
provided timely security updates, and will likely reduce costs for our customers. &lt;/p&gt;</content><category term="HPC"></category><category term="hpc"></category><category term="linux"></category><category term="provisioning"></category></entry><entry><title>The Impact of RHEL Changes on an HPC MSP</title><link href="https://blog.kail.io/the-impact-of-rhel-changes-on-an-hpc-msp.html" rel="alternate"></link><published>2023-06-27T12:00:00-04:00</published><updated>2023-06-27T12:00:00-04:00</updated><author><name>Andrew Kail</name></author><id>tag:blog.kail.io,2023-06-27:/the-impact-of-rhel-changes-on-an-hpc-msp.html</id><summary type="html">&lt;p&gt;Its been nearly a week now and after having stewed on it I think its time to speak my piece on this debacle.&lt;/p&gt;
&lt;p&gt;The recent changes to how Red Hat Enterprise Linux (RHEL) distributes their
source code has caused quite a stir in the Linux and HPC community.  &lt;/p&gt;
&lt;p&gt;While I â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Its been nearly a week now and after having stewed on it I think its time to speak my piece on this debacle.&lt;/p&gt;
&lt;p&gt;The recent changes to how Red Hat Enterprise Linux (RHEL) distributes their
source code has caused quite a stir in the Linux and HPC community.  &lt;/p&gt;
&lt;p&gt;While I can't fault RedHat/IBM for the business decision they have made, it does
leave us in a bit of a pickle with regard to the support we provide long term.&lt;/p&gt;
&lt;h2&gt;What Happened&lt;/h2&gt;
&lt;p&gt;This feels like beating a dead horse really, so I'll just leave a few links on the situation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/furthering-evolution-centos-stream"&gt;The initial announcement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/red-hats-commitment-open-source-response-gitcentosorg-changes"&gt;The RedHat response to Cricism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rocky Linux &lt;a href="https://rockylinux.org/news/2023-06-22-press-release/"&gt;1&lt;/a&gt; &lt;a href="https://rockylinux.org/news/brave-new-world-path-forward/"&gt;2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://almalinux.org/blog/impact-of-rhel-changes/"&gt;Alma Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sfconservancy.org/blog/2023/jun/23/rhel-gpl-analysis/"&gt;Software Freedom Conservancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jeff Geerling &lt;a href="https://www.jeffgeerling.com/blog/2023/dear-red-hat-are-you-dumb"&gt;1&lt;/a&gt; &lt;a href="https://www.jeffgeerling.com/blog/2023/removing-official-support-red-hat-enterprise-linux"&gt;2&lt;/a&gt; &lt;a href="https://www.jeffgeerling.com/blog/2023/im-done-red-hat-enterprise-linux"&gt;3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;tldr&lt;/strong&gt; - RedHad sees no value in allowing rebuilds, so they're closing it off.&lt;/p&gt;
&lt;h2&gt;How This Impacts Us&lt;/h2&gt;
&lt;p&gt;At the moment it doesn't thanks to the engineering teams at CIQ stepping up to provide continued updates which we are extremelly grateful for.  We are currently in the process of migrating customers from CentOS 7 clusters to Rocky 8 or 9 depending on the environment, with plans to migrate several more over the course of the year. This, of course, excludes the several we have that are running RHEL which will not be impacted at all by the change.&lt;/p&gt;
&lt;p&gt;The big question is how long can CIQ keep these updates going? Or how long can Alma maintain cherry picking updates out of CentOS Stream? Once RedHat realizes they didn't kill off the derivative rebuilds will they pursue other legal or more drastic actions?  Going a step further, what about other open source projects from RedHat?  Looking at you Ansible, Podman, etc.&lt;/p&gt;
&lt;p&gt;And what will IBM be doing for their products running on RHEL and OpenShift?  I'm worried about IBM Storage Scale (GPFS) only support clients on RHEL.&lt;/p&gt;
&lt;p&gt;The bigger blow for our engineering or is Jeff Geerling dropping support for Enterprise Linux with his ansible roles,
which are used heavily for some of our deployments.  Who will follow in dropping support?  We've already seen some application developers, namely GROMACS, state they will only officially support container stacks.&lt;/p&gt;
&lt;p&gt;Then, for so many of our clients, they can't absorb the licensing costs for the number of licenses they need.  Many are smaller clients on a budget and a huge increase in license costs will reduce the value they can provide to their users.  &lt;/p&gt;
&lt;h2&gt;What next&lt;/h2&gt;
&lt;p&gt;We honestly don't know what the future holds for us in our space.  Most of our clients are on CentOS, Rocky, or RHEL spanning multiple versions.  Most of our storage stack are supported only on Enterprise Linux or heavily favor it.  If Rocky or Alma eventually go away we'll have to make some decisions on those clients.&lt;/p&gt;
&lt;p&gt;In the meantime, we'll be prioritizing containerization of user applications via Singularity, Shifter, or CharlieCloud (Upcoming Article on this), as well as experiment with containerization of our supporting infrastructure.  While some pieces like the majority of the monitoring stack are easy to containerize, I am even looking at moving tools like Warewulf to a container which should be interesting.&lt;/p&gt;
&lt;p&gt;Our other option is to migrate some of our customer base to OpenSuse Leap and offer paid support if they need it. Leap is the CentOS/Rocky equiavalent of Suse Enterprise Linux (SLES) and is maintained directly by Suse.  We've met with their team in the past and have dedicated resources and packages for HPC environments.  I think this is their opportunity to step into the HPC space for many institutions and I really hope they don't miss it.  I'll be exploring openSuseLeap and warewulf in the coming weeks with some blog posts I hope.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This announcement hit me right before bed unfortunately what followed was not a good night's sleep as I wrestled with the implications.&lt;/p&gt;
&lt;p&gt;We'll be fine for now and we have some work to do to support our customers in the long term. &lt;/p&gt;
&lt;p&gt;Also, what they have done doesn't appear to be illegal nor violate the GPL license. Time will tell, but I'm starting to see some bleedover of Big Blue into RedHat more and more and have deep concerns about how we can support our clients long term. I hope I'm wrong.&lt;/p&gt;</content><category term="HPC"></category><category term="hpc"></category><category term="linux"></category></entry></feed>